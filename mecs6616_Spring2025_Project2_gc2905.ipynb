{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pC8PaXzq2js"
      },
      "source": [
        "# **MECS6616 Spring 2025 - Project 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inY7y5CRo97q"
      },
      "source": [
        "# **Introduction**\n",
        "\n",
        "***IMPORTANT:***\n",
        "- **Before starting, make sure to read the [Assignment Instructions](https://courseworks2.columbia.edu/courses/197115/pages/assignment-instructions) page on Courseworks to understand the workflow and submission requirements for this project.**\n",
        "\n",
        "This project aims to demonstrate how neural networks can be used in a robotics setting. We will continue using the 2D maze environment introduced in Project 1 and learn to navigate an agent to a goal. Since neural networks can be more powerful models than the ones we had access to in Project 1, we can afford to make some changes to the 2D maze environment and make the problem more difficult. The project is divided into three parts: In Part I, you will train a simple Deep Neural Network (DNN) to predict the optimal action towards the goal given the agent position and the goal position. In Parts II and III, you will train Convolutional Neural Networks (CNNs) to predict the optimal action given images of the maze environment.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://github.com/roamlab/robot-learning-S2023/blob/main/project2/imgs/P1_side.png?raw=true\" width=\"300\"/>\n",
        "</div>\n",
        "\n",
        "The figure above illustrates the simulation world, where the \"robot\" (also referred to as \"agent\") is represented by a green dot, and the goal location is marked by a red square. The agent's objective is to navigate to this goal location, avoiding any obstacles (depicted as black boxes) along the way.\n",
        "\n",
        "**Unlike the previous project, the robot and the goal are spawned at random positions in the maze.** Also, the action space now contains all four directions: 'up', 'down', 'left' and 'right'. Another change is that, in addition to the obstacle map shown above, we introduce two new obstacle maps as shown below. However, these new maps will not be used until Part III.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://github.com/roamlab/robot-learning-S2023/blob/main/project2/imgs/map1.png?raw=true\" width=\"300\"/>\n",
        "<img src=\"https://github.com/roamlab/robot-learning-S2023/blob/main/project2/imgs/map2.png?raw=true\" width=\"300\"/>\n",
        "<img src=\"https://github.com/roamlab/robot-learning-S2023/blob/main/project2/imgs/map3.png?raw=true\" width=\"300\"/>\n",
        "</div>\n",
        "\n",
        "We want to learn to navigate the agent by imitating demonstrations from an expert user. In all three parts, you will be using data collected by a human controlling the agent via a keyboard for training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1vjDH2fL9Tu"
      },
      "source": [
        "# **Project Setup (do NOT change)**\n",
        "\n",
        "***IMPORTANT:***\n",
        "- Do NOT change this \"*Project Setup*\" section\n",
        "- Do NOT install any other dependencies or a different version of an already provided package. You may, however, import other packages\n",
        "- Your code should go under the subsequent sections with headings \"*Part 1*\", \"*Part 2*\", and \"*Part 3*\"\n",
        "- You may find it useful to minimize sections using the arrows located to the left of each section heading\n",
        "- You may not use pre-trained models or any form of transfer learning for Part 2 and Part 3\n",
        "\n",
        "You will be accessing data files located in a Google Drive folder. The following cell downloads the data from the cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DptpWqf9awi7",
        "outputId": "7f0197cc-cfd0-4a77-e20d-f34d0a18ecc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-11 19:10:22--  https://www.dropbox.com/scl/fi/gy1d0ifkwuusmdjv796dl/project2.zip?rlkey=h6wresrsqxiryhlvrssjla5hn\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uca4329450164fe44cc525ac05f3.dl.dropboxusercontent.com/cd/0/inline/ClqTBPAjSb-q1TC1Dd7AcrA7UVLpC9k-gZQmHDdKqoYec0Gwm3Ax82J6eLVWD4nZAYTRoBEDCDIXN-Q7yN5hTfVSAJx0CAc3np3g07P9dk6KXrxfkZ8Xa_FU13Gn2ZLUX1xpNhyxnxcj1qRyGGKbs4NT/file# [following]\n",
            "--2025-03-11 19:10:22--  https://uca4329450164fe44cc525ac05f3.dl.dropboxusercontent.com/cd/0/inline/ClqTBPAjSb-q1TC1Dd7AcrA7UVLpC9k-gZQmHDdKqoYec0Gwm3Ax82J6eLVWD4nZAYTRoBEDCDIXN-Q7yN5hTfVSAJx0CAc3np3g07P9dk6KXrxfkZ8Xa_FU13Gn2ZLUX1xpNhyxnxcj1qRyGGKbs4NT/file\n",
            "Resolving uca4329450164fe44cc525ac05f3.dl.dropboxusercontent.com (uca4329450164fe44cc525ac05f3.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to uca4329450164fe44cc525ac05f3.dl.dropboxusercontent.com (uca4329450164fe44cc525ac05f3.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/Clp86j7dqO7JG_q61SeXV9xDW63exL4wa6Alr62jlae7EKkgmmVZ4iCzkKYKZIf2o4-nGXZuVGFrKbqA_4t8XSluAHCFSoGCq2v6brcTOfGWTBCmcmbKEgBF3NqH3u68kKxUwxyS_yr31pIzZZWF4qdcX3ZCT4n8gj7D2ICsh1OgdZxuyC3lOGt-sGKrXRMeSX4i6-9BoZApNpk6m1kKcxzSzQCYSepouzgy6U-cnPdGWF6aSbiJaCzXvUpVCYc_rY85_icSIfwN7PiBjqgIL4d2KGK1ER4E-k7AQ0omQzBjWHVoyKKj_oIRAM2DupYM2DmaPzpz6UGI8bk0MP8NwteVA9NG6WImzI0Zh9qPwFx2BohHQy_dZsS4bNLOmraLTnc/file [following]\n",
            "--2025-03-11 19:10:23--  https://uca4329450164fe44cc525ac05f3.dl.dropboxusercontent.com/cd/0/inline2/Clp86j7dqO7JG_q61SeXV9xDW63exL4wa6Alr62jlae7EKkgmmVZ4iCzkKYKZIf2o4-nGXZuVGFrKbqA_4t8XSluAHCFSoGCq2v6brcTOfGWTBCmcmbKEgBF3NqH3u68kKxUwxyS_yr31pIzZZWF4qdcX3ZCT4n8gj7D2ICsh1OgdZxuyC3lOGt-sGKrXRMeSX4i6-9BoZApNpk6m1kKcxzSzQCYSepouzgy6U-cnPdGWF6aSbiJaCzXvUpVCYc_rY85_icSIfwN7PiBjqgIL4d2KGK1ER4E-k7AQ0omQzBjWHVoyKKj_oIRAM2DupYM2DmaPzpz6UGI8bk0MP8NwteVA9NG6WImzI0Zh9qPwFx2BohHQy_dZsS4bNLOmraLTnc/file\n",
            "Reusing existing connection to uca4329450164fe44cc525ac05f3.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13335134 (13M) [application/zip]\n",
            "Saving to: ‘project2.zip?rlkey=h6wresrsqxiryhlvrssjla5hn’\n",
            "\n",
            "project2.zip?rlkey= 100%[===================>]  12.72M  49.5MB/s    in 0.3s    \n",
            "\n",
            "2025-03-11 19:10:23 (49.5 MB/s) - ‘project2.zip?rlkey=h6wresrsqxiryhlvrssjla5hn’ saved [13335134/13335134]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# DO NOT CHANGE\n",
        "# Download data\n",
        "!wget https://www.dropbox.com/scl/fi/gy1d0ifkwuusmdjv796dl/project2.zip?rlkey=h6wresrsqxiryhlvrssjla5hn&st=cfvqccqm&dl=0\n",
        "!mv project2.zip?rlkey=h6wresrsqxiryhlvrssjla5hn project2.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IYctlC5-gfU",
        "outputId": "64be2a42-5e83-4cb9-9fd4-8b8d465c263f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  project2.zip\n",
            "   creating: /content/mjcf/\n",
            "  inflating: /content/mjcf/point_mass.xml  \n",
            "   creating: /content/mjcf/common/\n",
            "  inflating: /content/mjcf/common/skybox.xml  \n",
            "  inflating: /content/mjcf/common/visual.xml  \n",
            "  inflating: /content/mjcf/common/materials.xml  \n",
            "  inflating: /content/mjcf/test_mjcf.xml  \n",
            "  inflating: /content/dnn.py         \n",
            "   creating: /content/imgs/\n",
            "  inflating: /content/imgs/P1_side.png  \n",
            "  inflating: /content/imgs/map1.png  \n",
            "  inflating: /content/imgs/map3.png  \n",
            "  inflating: /content/imgs/map2.png  \n",
            "  inflating: /content/score_policy.py  \n",
            "  inflating: /content/simple_maze.py  \n",
            "  inflating: /content/data_utils.py  \n",
            "   creating: /content/data/\n",
            "  inflating: /content/data/map1.pkl  \n",
            "  inflating: /content/data/all_maps.pkl  \n"
          ]
        }
      ],
      "source": [
        "# Make sure you have successfully uploaded the zip file to Colab before running the line below.\n",
        "# If wget fails to pull the zip file, you can download the zipfile from dropbox and manually upload it to collab instead\n",
        "# If you do decide to manually upload the file, use the dropbox link in the previous cell (after wget) to access the file\n",
        "# Make sure the zip file is named \"project2.zip\", rename it before uploading (if necessary)\n",
        "# Upload the entire zip file to google colab. Do not unzip before uploading\n",
        "\n",
        "# Unzip the uploaded zip file\n",
        "!unzip -o project2.zip -d /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2w_kfLOMFIG",
        "outputId": "7bdb55bb-614e-473e-f9b0-debece89a206"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pybullet\n",
            "  Downloading pybullet-3.2.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting numpngw\n",
            "  Downloading numpngw-0.1.4-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from numpngw) (1.26.4)\n",
            "Downloading pybullet-3.2.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (103.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpngw-0.1.4-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: pybullet, numpngw\n",
            "Successfully installed numpngw-0.1.4 pybullet-3.2.7\n"
          ]
        }
      ],
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "# Install required packages\n",
        "!pip install pybullet numpngw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wz0ALBOu6coY"
      },
      "source": [
        "# Part I. Behavioral cloning with low dimensional data\n",
        "\n",
        "This part is a natural extension of Part II in Project 1, where your agent needs to learn a policy using labeled examples from an expert.\n",
        "\n",
        "Each labeled example $i$ will contain a tuple of the form $(o, a)^i$, where $o$ represents an observation and $a$ represents the action taken by the expert given that observation. You must simply learn to imitate the expert, a process also known as behavioral cloning. Note that while the observation space will be different in each part, the action space is the same for the rest of the project.\n",
        "\n",
        "We will be training a DNN policy to predict an action to be taken ('up', 'down', 'left', and 'right') based on the observation. **In Part I, the observation will contain the agent position and the current goal position.** (Since the goal is sampled randomly, the policy has to know the current goal to be reached). The environment thus returns an observation array of size (4, ) where the agent position is contained in the first two axes and the current goal position is contained in the next two. **In Part I, the map that the robot is navigating is always the same.**\n",
        "\n",
        "PyTorch and Tensorflow are two popular frameworks for building and training neural networks but for this class, we will be exclusively using PyTorch and you are allowed to use any of its features. A good starting point can be found [here](https://github.com/roamlab/robot-learning-S2024/blob/main/dnn_example.py).\n",
        "\n",
        "You will implement a class that inherits from `RobotPolicy` by providing implementations for the abstract methods from the class. These abstract methods will be re-used by future parts of the project, so do not edit them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtOnOJ3Bci6g"
      },
      "source": [
        "**NOTES:**\n",
        "- The problem is about behavioral cloning which means we want to train a deep neural network - so at least one hidden layer- to imitate an expert's actions based on observations.\n",
        "\n",
        "- Agents position: (x,y)\n",
        "- Goal's position (x,y)\n",
        "\n",
        "- Observations: 4D vector: (4000, 4)\n",
        "  - First few X values:\n",
        "  - The first two are the agents position\n",
        "  - the last two are the goals position\n",
        "\n",
        "[[-0.38606754 -0.37158364  0.0612401   0.38303697]\n",
        "\n",
        " [-0.38606754 -0.28158364  0.0612401   0.38303697]\n",
        "\n",
        " [-0.38606754 -0.18158363  0.0612401   0.38303697]\n",
        "\n",
        " [-0.38606754 -0.08158363  0.0612401   0 38303697]\n",
        "\n",
        " [-0.38606754  0.01841637  0.0612401   0.38303697]]\n",
        "\n",
        "- Actions: (4000,)\n",
        "  - (0) up, (1) down, (2) left, (3) right\n",
        "  - classification so we can use cross entropy loss as the loss function\n",
        "\n",
        "**Neural Network Architecture:**\n",
        "\n",
        "- Input:\n",
        "  - 4 neurons, one for each of the current states: agent and goal position\n",
        "- Hidden Layers:\n",
        "  - simplest 1:\n",
        "  - two hidden layers:\n",
        "  - ReLU activation function\n",
        "\n",
        "\n",
        "- output:\n",
        "  - fully connected so 4 output neurons\n",
        "  - Softmax for output\n",
        "\n",
        "- loss function:\n",
        "  - Cross entropy loss : classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAb3tE7h9uUd"
      },
      "outputs": [],
      "source": [
        "# DO NOT CHANGE\n",
        "# base class\n",
        "\n",
        "import abc\n",
        "\n",
        "\n",
        "class RobotPolicy(abc.ABC):\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def train(self, data):\n",
        "        \"\"\"\n",
        "            Abstract method for training a policy.\n",
        "\n",
        "            Args:\n",
        "                data: a dict that contains X (key = 'obs') and y (key = 'actions').\n",
        "\n",
        "                X is either rgb image (N, 64, 64, 3) OR  agent & goal pos (N, 4)\n",
        "\n",
        "            Returns:\n",
        "                This method does not return anything. It will just need to update the\n",
        "                property of a RobotPolicy instance.\n",
        "        \"\"\"\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def get_action(self, obs):\n",
        "        \"\"\"\n",
        "            Abstract method for getting action. You can do data preprocessing and feed\n",
        "            forward of your trained model here.\n",
        "            Args:\n",
        "                obs: an observation (64 x 64 x 3) rgb image OR (4, ) positions\n",
        "\n",
        "            Returns:\n",
        "                action: an integer between 0 to 3\n",
        "        \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZH35w1j-Y7t3"
      },
      "outputs": [],
      "source": [
        "# Implement your solution for Part 1 below\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import random\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "\n",
        "def set_seed(seed=42):\n",
        "   torch.manual_seed(seed)\n",
        "   torch.cuda.manual_seed_all(seed)  # Even though CPU, this doesn't hurt.\n",
        "   np.random.seed(seed\n",
        ")\n",
        "   random.seed(seed)\n",
        "   torch.backends.cudnn.deterministic = True\n",
        "   torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "\n",
        "class POSBCRobot(RobotPolicy):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "      super().__init__()\n",
        "\n",
        "      self.model = nn.Sequential(\n",
        "          #4 features to n^5 neurons in hidden layer\n",
        "          nn.Linear(4,32),\n",
        "          #normalizes activations\n",
        "          nn.BatchNorm1d(32),\n",
        "          #Activation function\n",
        "          nn.ReLU(),\n",
        "          #second hidden layer\n",
        "          nn.Linear(32, 64),\n",
        "          nn.BatchNorm1d(64),\n",
        "          nn.ReLU(),\n",
        "          #third hidden layer\n",
        "          nn.Linear(64, 32),\n",
        "          nn.BatchNorm1d(32),\n",
        "          #activation function 2\n",
        "          nn.ReLU(),\n",
        "          #32 to 4 output\n",
        "          nn.Linear(32,4)\n",
        "      )\n",
        "\n",
        "      self.criterion = nn.CrossEntropyLoss()\n",
        "      self.optimizer = optim.AdamW(self.model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "\n",
        "    def train(self, data):\n",
        "\n",
        "        X_data = data['obs']\n",
        "        y_data = data['actions']\n",
        "\n",
        "        #formatting into tensors for pytorch\n",
        "\n",
        "        X = torch.tensor(data['obs'], dtype=torch.float32)\n",
        "        y = torch.tensor(data['actions'], dtype = torch.long)\n",
        "\n",
        "        #trainig loop\n",
        "\n",
        "        num_epochs = 800\n",
        "\n",
        "        #this is where we update the models weights using back propagation and gradient decent\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "\n",
        "          #this clears the gradients before computing new ones\n",
        "            self.optimizer.zero_grad()\n",
        "          #input data --> the model will predict logits for each action\n",
        "            outputs = self.model(X)\n",
        "\n",
        "          #How confident the model is about taking each action --> it'll pick the highest prob\n",
        "          #outputs = tensor([[-1.23, 2.56, 0.87, -0.41]])\n",
        "          #compare's the models. predictions to true labels\n",
        "\n",
        "            loss = self.criterion(outputs,y)\n",
        "          #backpropagation computes the gradients\n",
        "            loss.backward()\n",
        "          #use the computed gradients to adjust model params\n",
        "            self.optimizer.step()\n",
        "\n",
        "            if epoch % 2 == 0:\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "    def get_action(self, obs):\n",
        "        obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)  # Convert to tensor & add batch dimension\n",
        "\n",
        "\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():  # Disable gradient computation\n",
        "            output = self.model(obs_tensor)  # Get action scores\n",
        "            action = torch.argmax(output).item()  # Pick the action with the highest score\n",
        "\n",
        "\n",
        "\n",
        "        self.model.train()\n",
        "        return action\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLchnZGWZYeP"
      },
      "source": [
        "## Evaluation and Grading\n",
        "\n",
        "We will evaluate your model by simply having the agent follow the commands that it provides.  We will evaluate for 100 different randomly sampled starting positions and goals. For each goal, we roll out the trained policy for 50 steps. After the 50 steps, we will evaluate the closest distance to the goal the agent has ended up. If the agent reaches < 0.1 distance from the goal, the episode is ended before 50 steps and the minimum distance will be recorded as 0. The score is the fraction of the initial distance to goal covered by the agent averaged over 100 trials. Your final grade will be computed based on this score.\n",
        "\n",
        "We will calculate the score using the formula :\n",
        "\n",
        "```score = avg[(init_dist -  min_dist) / init_dist]```\n",
        "\n",
        "We will auto-generate your grades using the code below. The grading of each part is separate from each other so you can get the grade right after each part is finished.\n",
        "\n",
        "The total points of this assignment are 15. According to the difficulty level of each part, parts 1, 2, and 3 have 4, 5, 6 points respectively.\n",
        "\n",
        "- Part 1: if your score >= 0.99, you will receive 4 / 4. Otherwise, your final grade will be score / 0.99 * 4.\n",
        "- Part 2: if your score >= 0.95, you will receive 5 / 5. Otherwise, your final grade will be score / 0.95 * 5.\n",
        "- Part 3: if your score >= 0.95, you will receive 6 / 6. Otherwise, your final grade will be score / 0.95 * 6.\n",
        "\n",
        "The score function for each part provides two extra arguments to assist your debugging.\n",
        "\n",
        "- gui: If this is set to True, you will save the behavior of the agents during evaluation as an animation file. This animation file can be visualized using the provided code below to help you understand the behavior of the agent. **Please set it to False before your submission as it will slow down evaluation.**\n",
        "- model: If you provide a path to a saved model, the score function will not train from scratch but will instead load the save model. **Please set it to None before submission.** Any models you generate during runtime will be automatically deleted when disconnected. The grader will train the model from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZporTBmpmahZ"
      },
      "outputs": [],
      "source": [
        "# DO NOT CHANGE\n",
        "# Set up grading\n",
        "\n",
        "import score_policy\n",
        "import importlib\n",
        "importlib.reload(score_policy)\n",
        "from IPython.display import Image\n",
        "\n",
        "\n",
        "part1_bound = 0.99\n",
        "part2_bound = 0.95\n",
        "part3_bound = 0.95"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-c-Ob4uUlRM7",
        "outputId": "b5a53023-50ab-46b7-98f2-02ae20cdd9d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/800], Loss: 1.4681\n",
            "Epoch [3/800], Loss: 0.9381\n",
            "Epoch [5/800], Loss: 0.8178\n",
            "Epoch [7/800], Loss: 0.7304\n",
            "Epoch [9/800], Loss: 0.6542\n",
            "Epoch [11/800], Loss: 0.5911\n",
            "Epoch [13/800], Loss: 0.5326\n",
            "Epoch [15/800], Loss: 0.4871\n",
            "Epoch [17/800], Loss: 0.4504\n",
            "Epoch [19/800], Loss: 0.4203\n",
            "Epoch [21/800], Loss: 0.3960\n",
            "Epoch [23/800], Loss: 0.3762\n",
            "Epoch [25/800], Loss: 0.3610\n",
            "Epoch [27/800], Loss: 0.3484\n",
            "Epoch [29/800], Loss: 0.3375\n",
            "Epoch [31/800], Loss: 0.3270\n",
            "Epoch [33/800], Loss: 0.3170\n",
            "Epoch [35/800], Loss: 0.3060\n",
            "Epoch [37/800], Loss: 0.2965\n",
            "Epoch [39/800], Loss: 0.2870\n",
            "Epoch [41/800], Loss: 0.2775\n",
            "Epoch [43/800], Loss: 0.2675\n",
            "Epoch [45/800], Loss: 0.2582\n",
            "Epoch [47/800], Loss: 0.2493\n",
            "Epoch [49/800], Loss: 0.2401\n",
            "Epoch [51/800], Loss: 0.2324\n",
            "Epoch [53/800], Loss: 0.2348\n",
            "Epoch [55/800], Loss: 0.2288\n",
            "Epoch [57/800], Loss: 0.2228\n",
            "Epoch [59/800], Loss: 0.2102\n",
            "Epoch [61/800], Loss: 0.2016\n",
            "Epoch [63/800], Loss: 0.1943\n",
            "Epoch [65/800], Loss: 0.1911\n",
            "Epoch [67/800], Loss: 0.1879\n",
            "Epoch [69/800], Loss: 0.1799\n",
            "Epoch [71/800], Loss: 0.1728\n",
            "Epoch [73/800], Loss: 0.1718\n",
            "Epoch [75/800], Loss: 0.1727\n",
            "Epoch [77/800], Loss: 0.1750\n",
            "Epoch [79/800], Loss: 0.1642\n",
            "Epoch [81/800], Loss: 0.1624\n",
            "Epoch [83/800], Loss: 0.1571\n",
            "Epoch [85/800], Loss: 0.1534\n",
            "Epoch [87/800], Loss: 0.1516\n",
            "Epoch [89/800], Loss: 0.1521\n",
            "Epoch [91/800], Loss: 0.1590\n",
            "Epoch [93/800], Loss: 0.1424\n",
            "Epoch [95/800], Loss: 0.1504\n",
            "Epoch [97/800], Loss: 0.1454\n",
            "Epoch [99/800], Loss: 0.1345\n",
            "Epoch [101/800], Loss: 0.1340\n",
            "Epoch [103/800], Loss: 0.1357\n",
            "Epoch [105/800], Loss: 0.1337\n",
            "Epoch [107/800], Loss: 0.1300\n",
            "Epoch [109/800], Loss: 0.1507\n",
            "Epoch [111/800], Loss: 0.1361\n",
            "Epoch [113/800], Loss: 0.1421\n",
            "Epoch [115/800], Loss: 0.1288\n",
            "Epoch [117/800], Loss: 0.1211\n",
            "Epoch [119/800], Loss: 0.1210\n",
            "Epoch [121/800], Loss: 0.1223\n",
            "Epoch [123/800], Loss: 0.1188\n",
            "Epoch [125/800], Loss: 0.1127\n",
            "Epoch [127/800], Loss: 0.1098\n",
            "Epoch [129/800], Loss: 0.1091\n",
            "Epoch [131/800], Loss: 0.1057\n",
            "Epoch [133/800], Loss: 0.1063\n",
            "Epoch [135/800], Loss: 0.1210\n",
            "Epoch [137/800], Loss: 0.1366\n",
            "Epoch [139/800], Loss: 0.1042\n",
            "Epoch [141/800], Loss: 0.1144\n",
            "Epoch [143/800], Loss: 0.1176\n",
            "Epoch [145/800], Loss: 0.1109\n",
            "Epoch [147/800], Loss: 0.1032\n",
            "Epoch [149/800], Loss: 0.1025\n",
            "Epoch [151/800], Loss: 0.1018\n",
            "Epoch [153/800], Loss: 0.0971\n",
            "Epoch [155/800], Loss: 0.0974\n",
            "Epoch [157/800], Loss: 0.0988\n",
            "Epoch [159/800], Loss: 0.0897\n",
            "Epoch [161/800], Loss: 0.0908\n",
            "Epoch [163/800], Loss: 0.0933\n",
            "Epoch [165/800], Loss: 0.0862\n",
            "Epoch [167/800], Loss: 0.0935\n",
            "Epoch [169/800], Loss: 0.0943\n",
            "Epoch [171/800], Loss: 0.1105\n",
            "Epoch [173/800], Loss: 0.0974\n",
            "Epoch [175/800], Loss: 0.0926\n",
            "Epoch [177/800], Loss: 0.0946\n",
            "Epoch [179/800], Loss: 0.0923\n",
            "Epoch [181/800], Loss: 0.0918\n",
            "Epoch [183/800], Loss: 0.0848\n",
            "Epoch [185/800], Loss: 0.0785\n",
            "Epoch [187/800], Loss: 0.0762\n",
            "Epoch [189/800], Loss: 0.0789\n",
            "Epoch [191/800], Loss: 0.0769\n",
            "Epoch [193/800], Loss: 0.0874\n",
            "Epoch [195/800], Loss: 0.0817\n",
            "Epoch [197/800], Loss: 0.0933\n",
            "Epoch [199/800], Loss: 0.0754\n",
            "Epoch [201/800], Loss: 0.0867\n",
            "Epoch [203/800], Loss: 0.0772\n",
            "Epoch [205/800], Loss: 0.0821\n",
            "Epoch [207/800], Loss: 0.1157\n",
            "Epoch [209/800], Loss: 0.0967\n",
            "Epoch [211/800], Loss: 0.0897\n",
            "Epoch [213/800], Loss: 0.0824\n",
            "Epoch [215/800], Loss: 0.0813\n",
            "Epoch [217/800], Loss: 0.0757\n",
            "Epoch [219/800], Loss: 0.0683\n",
            "Epoch [221/800], Loss: 0.0699\n",
            "Epoch [223/800], Loss: 0.0686\n",
            "Epoch [225/800], Loss: 0.0702\n",
            "Epoch [227/800], Loss: 0.0753\n",
            "Epoch [229/800], Loss: 0.0684\n",
            "Epoch [231/800], Loss: 0.0586\n",
            "Epoch [233/800], Loss: 0.0615\n",
            "Epoch [235/800], Loss: 0.0585\n",
            "Epoch [237/800], Loss: 0.0564\n",
            "Epoch [239/800], Loss: 0.0540\n",
            "Epoch [241/800], Loss: 0.0529\n",
            "Epoch [243/800], Loss: 0.0567\n",
            "Epoch [245/800], Loss: 0.1388\n",
            "Epoch [247/800], Loss: 0.1621\n",
            "Epoch [249/800], Loss: 0.1532\n",
            "Epoch [251/800], Loss: 0.1321\n",
            "Epoch [253/800], Loss: 0.1235\n",
            "Epoch [255/800], Loss: 0.1091\n",
            "Epoch [257/800], Loss: 0.0959\n",
            "Epoch [259/800], Loss: 0.0877\n",
            "Epoch [261/800], Loss: 0.0874\n",
            "Epoch [263/800], Loss: 0.0810\n",
            "Epoch [265/800], Loss: 0.0740\n",
            "Epoch [267/800], Loss: 0.0719\n",
            "Epoch [269/800], Loss: 0.0687\n",
            "Epoch [271/800], Loss: 0.0675\n",
            "Epoch [273/800], Loss: 0.0632\n",
            "Epoch [275/800], Loss: 0.0591\n",
            "Epoch [277/800], Loss: 0.0587\n",
            "Epoch [279/800], Loss: 0.0562\n",
            "Epoch [281/800], Loss: 0.0541\n",
            "Epoch [283/800], Loss: 0.0516\n",
            "Epoch [285/800], Loss: 0.0499\n",
            "Epoch [287/800], Loss: 0.0491\n",
            "Epoch [289/800], Loss: 0.0475\n",
            "Epoch [291/800], Loss: 0.0459\n",
            "Epoch [293/800], Loss: 0.0450\n",
            "Epoch [295/800], Loss: 0.0436\n",
            "Epoch [297/800], Loss: 0.0428\n",
            "Epoch [299/800], Loss: 0.0416\n",
            "Epoch [301/800], Loss: 0.0413\n",
            "Epoch [303/800], Loss: 0.0416\n",
            "Epoch [305/800], Loss: 0.0503\n",
            "Epoch [307/800], Loss: 0.1659\n",
            "Epoch [309/800], Loss: 0.1217\n",
            "Epoch [311/800], Loss: 0.1137\n",
            "Epoch [313/800], Loss: 0.1439\n",
            "Epoch [315/800], Loss: 0.1621\n",
            "Epoch [317/800], Loss: 0.1229\n",
            "Epoch [319/800], Loss: 0.1232\n",
            "Epoch [321/800], Loss: 0.0999\n",
            "Epoch [323/800], Loss: 0.1006\n",
            "Epoch [325/800], Loss: 0.0902\n",
            "Epoch [327/800], Loss: 0.0877\n",
            "Epoch [329/800], Loss: 0.0808\n",
            "Epoch [331/800], Loss: 0.0760\n",
            "Epoch [333/800], Loss: 0.0719\n",
            "Epoch [335/800], Loss: 0.0700\n",
            "Epoch [337/800], Loss: 0.0651\n",
            "Epoch [339/800], Loss: 0.0630\n",
            "Epoch [341/800], Loss: 0.0604\n",
            "Epoch [343/800], Loss: 0.0568\n",
            "Epoch [345/800], Loss: 0.0550\n",
            "Epoch [347/800], Loss: 0.0528\n",
            "Epoch [349/800], Loss: 0.0507\n",
            "Epoch [351/800], Loss: 0.0490\n",
            "Epoch [353/800], Loss: 0.0475\n",
            "Epoch [355/800], Loss: 0.0459\n",
            "Epoch [357/800], Loss: 0.0447\n",
            "Epoch [359/800], Loss: 0.0432\n",
            "Epoch [361/800], Loss: 0.0420\n",
            "Epoch [363/800], Loss: 0.0407\n",
            "Epoch [365/800], Loss: 0.0397\n",
            "Epoch [367/800], Loss: 0.0387\n",
            "Epoch [369/800], Loss: 0.0377\n",
            "Epoch [371/800], Loss: 0.0370\n",
            "Epoch [373/800], Loss: 0.0363\n",
            "Epoch [375/800], Loss: 0.0360\n",
            "Epoch [377/800], Loss: 0.0357\n",
            "Epoch [379/800], Loss: 0.0347\n",
            "Epoch [381/800], Loss: 0.0344\n",
            "Epoch [383/800], Loss: 0.0334\n",
            "Epoch [385/800], Loss: 0.0329\n",
            "Epoch [387/800], Loss: 0.0324\n",
            "Epoch [389/800], Loss: 0.0310\n",
            "Epoch [391/800], Loss: 0.0317\n",
            "Epoch [393/800], Loss: 0.0481\n",
            "Epoch [395/800], Loss: 0.2914\n",
            "Epoch [397/800], Loss: 0.1878\n",
            "Epoch [399/800], Loss: 0.2262\n",
            "Epoch [401/800], Loss: 0.1678\n",
            "Epoch [403/800], Loss: 0.1434\n",
            "Epoch [405/800], Loss: 0.1192\n",
            "Epoch [407/800], Loss: 0.1137\n",
            "Epoch [409/800], Loss: 0.1102\n",
            "Epoch [411/800], Loss: 0.0942\n",
            "Epoch [413/800], Loss: 0.0946\n",
            "Epoch [415/800], Loss: 0.0860\n",
            "Epoch [417/800], Loss: 0.0811\n",
            "Epoch [419/800], Loss: 0.0758\n",
            "Epoch [421/800], Loss: 0.0731\n",
            "Epoch [423/800], Loss: 0.0679\n",
            "Epoch [425/800], Loss: 0.0650\n",
            "Epoch [427/800], Loss: 0.0633\n",
            "Epoch [429/800], Loss: 0.0595\n",
            "Epoch [431/800], Loss: 0.0579\n",
            "Epoch [433/800], Loss: 0.0552\n",
            "Epoch [435/800], Loss: 0.0530\n",
            "Epoch [437/800], Loss: 0.0510\n",
            "Epoch [439/800], Loss: 0.0494\n",
            "Epoch [441/800], Loss: 0.0479\n",
            "Epoch [443/800], Loss: 0.0459\n",
            "Epoch [445/800], Loss: 0.0446\n",
            "Epoch [447/800], Loss: 0.0433\n",
            "Epoch [449/800], Loss: 0.0419\n",
            "Epoch [451/800], Loss: 0.0407\n",
            "Epoch [453/800], Loss: 0.0395\n",
            "Epoch [455/800], Loss: 0.0384\n",
            "Epoch [457/800], Loss: 0.0374\n",
            "Epoch [459/800], Loss: 0.0363\n",
            "Epoch [461/800], Loss: 0.0353\n",
            "Epoch [463/800], Loss: 0.0344\n",
            "Epoch [465/800], Loss: 0.0336\n",
            "Epoch [467/800], Loss: 0.0329\n",
            "Epoch [469/800], Loss: 0.0321\n",
            "Epoch [471/800], Loss: 0.0314\n",
            "Epoch [473/800], Loss: 0.0306\n",
            "Epoch [475/800], Loss: 0.0301\n",
            "Epoch [477/800], Loss: 0.0294\n",
            "Epoch [479/800], Loss: 0.0289\n",
            "Epoch [481/800], Loss: 0.0283\n",
            "Epoch [483/800], Loss: 0.0277\n",
            "Epoch [485/800], Loss: 0.0273\n",
            "Epoch [487/800], Loss: 0.0268\n",
            "Epoch [489/800], Loss: 0.0263\n",
            "Epoch [491/800], Loss: 0.0258\n",
            "Epoch [493/800], Loss: 0.0254\n",
            "Epoch [495/800], Loss: 0.0251\n",
            "Epoch [497/800], Loss: 0.0253\n",
            "Epoch [499/800], Loss: 0.0252\n",
            "Epoch [501/800], Loss: 0.0239\n",
            "Epoch [503/800], Loss: 0.0244\n",
            "Epoch [505/800], Loss: 0.0257\n",
            "Epoch [507/800], Loss: 0.0236\n",
            "Epoch [509/800], Loss: 0.0264\n",
            "Epoch [511/800], Loss: 0.0233\n",
            "Epoch [513/800], Loss: 0.0251\n",
            "Epoch [515/800], Loss: 0.0251\n",
            "Epoch [517/800], Loss: 0.0227\n",
            "Epoch [519/800], Loss: 0.0280\n",
            "Epoch [521/800], Loss: 0.0255\n",
            "Epoch [523/800], Loss: 0.0440\n",
            "Epoch [525/800], Loss: 0.0323\n",
            "Epoch [527/800], Loss: 0.0309\n",
            "Epoch [529/800], Loss: 0.0489\n",
            "Epoch [531/800], Loss: 0.0377\n",
            "Epoch [533/800], Loss: 0.0514\n",
            "Epoch [535/800], Loss: 0.0535\n",
            "Epoch [537/800], Loss: 0.0466\n",
            "Epoch [539/800], Loss: 0.0499\n",
            "Epoch [541/800], Loss: 0.0442\n",
            "Epoch [543/800], Loss: 0.0436\n",
            "Epoch [545/800], Loss: 0.0395\n",
            "Epoch [547/800], Loss: 0.0346\n",
            "Epoch [549/800], Loss: 0.0315\n",
            "Epoch [551/800], Loss: 0.0287\n",
            "Epoch [553/800], Loss: 0.0266\n",
            "Epoch [555/800], Loss: 0.0252\n",
            "Epoch [557/800], Loss: 0.0234\n",
            "Epoch [559/800], Loss: 0.0223\n",
            "Epoch [561/800], Loss: 0.0219\n",
            "Epoch [563/800], Loss: 0.0209\n",
            "Epoch [565/800], Loss: 0.0198\n",
            "Epoch [567/800], Loss: 0.0193\n",
            "Epoch [569/800], Loss: 0.0190\n",
            "Epoch [571/800], Loss: 0.0183\n",
            "Epoch [573/800], Loss: 0.0177\n",
            "Epoch [575/800], Loss: 0.0172\n",
            "Epoch [577/800], Loss: 0.0170\n",
            "Epoch [579/800], Loss: 0.0167\n",
            "Epoch [581/800], Loss: 0.0164\n",
            "Epoch [583/800], Loss: 0.0160\n",
            "Epoch [585/800], Loss: 0.0158\n",
            "Epoch [587/800], Loss: 0.0156\n",
            "Epoch [589/800], Loss: 0.0154\n",
            "Epoch [591/800], Loss: 0.0151\n",
            "Epoch [593/800], Loss: 0.0149\n",
            "Epoch [595/800], Loss: 0.0147\n",
            "Epoch [597/800], Loss: 0.0145\n",
            "Epoch [599/800], Loss: 0.0143\n",
            "Epoch [601/800], Loss: 0.0142\n",
            "Epoch [603/800], Loss: 0.0140\n",
            "Epoch [605/800], Loss: 0.0139\n",
            "Epoch [607/800], Loss: 0.0138\n",
            "Epoch [609/800], Loss: 0.0137\n",
            "Epoch [611/800], Loss: 0.0134\n",
            "Epoch [613/800], Loss: 0.0133\n",
            "Epoch [615/800], Loss: 0.0132\n",
            "Epoch [617/800], Loss: 0.0131\n",
            "Epoch [619/800], Loss: 0.0129\n",
            "Epoch [621/800], Loss: 0.0127\n",
            "Epoch [623/800], Loss: 0.0126\n",
            "Epoch [625/800], Loss: 0.0124\n",
            "Epoch [627/800], Loss: 0.0126\n",
            "Epoch [629/800], Loss: 0.0129\n",
            "Epoch [631/800], Loss: 0.0124\n",
            "Epoch [633/800], Loss: 0.0120\n",
            "Epoch [635/800], Loss: 0.0123\n",
            "Epoch [637/800], Loss: 0.0142\n",
            "Epoch [639/800], Loss: 0.0155\n",
            "Epoch [641/800], Loss: 0.0124\n",
            "Epoch [643/800], Loss: 0.0182\n",
            "Epoch [645/800], Loss: 0.0319\n",
            "Epoch [647/800], Loss: 0.3028\n",
            "Epoch [649/800], Loss: 0.7532\n",
            "Epoch [651/800], Loss: 0.8590\n",
            "Epoch [653/800], Loss: 0.6127\n",
            "Epoch [655/800], Loss: 0.4838\n",
            "Epoch [657/800], Loss: 0.3696\n",
            "Epoch [659/800], Loss: 0.3262\n",
            "Epoch [661/800], Loss: 0.2815\n",
            "Epoch [663/800], Loss: 0.2489\n",
            "Epoch [665/800], Loss: 0.2201\n",
            "Epoch [667/800], Loss: 0.2170\n",
            "Epoch [669/800], Loss: 0.2064\n",
            "Epoch [671/800], Loss: 0.1985\n",
            "Epoch [673/800], Loss: 0.1850\n",
            "Epoch [675/800], Loss: 0.1782\n",
            "Epoch [677/800], Loss: 0.1743\n",
            "Epoch [679/800], Loss: 0.1681\n",
            "Epoch [681/800], Loss: 0.1631\n",
            "Epoch [683/800], Loss: 0.1589\n",
            "Epoch [685/800], Loss: 0.1554\n",
            "Epoch [687/800], Loss: 0.1513\n",
            "Epoch [689/800], Loss: 0.1480\n",
            "Epoch [691/800], Loss: 0.1446\n",
            "Epoch [693/800], Loss: 0.1410\n",
            "Epoch [695/800], Loss: 0.1387\n",
            "Epoch [697/800], Loss: 0.1361\n",
            "Epoch [699/800], Loss: 0.1332\n",
            "Epoch [701/800], Loss: 0.1307\n",
            "Epoch [703/800], Loss: 0.1283\n",
            "Epoch [705/800], Loss: 0.1259\n",
            "Epoch [707/800], Loss: 0.1235\n",
            "Epoch [709/800], Loss: 0.1214\n",
            "Epoch [711/800], Loss: 0.1191\n",
            "Epoch [713/800], Loss: 0.1171\n",
            "Epoch [715/800], Loss: 0.1151\n",
            "Epoch [717/800], Loss: 0.1132\n",
            "Epoch [719/800], Loss: 0.1114\n",
            "Epoch [721/800], Loss: 0.1096\n",
            "Epoch [723/800], Loss: 0.1079\n",
            "Epoch [725/800], Loss: 0.1062\n",
            "Epoch [727/800], Loss: 0.1046\n",
            "Epoch [729/800], Loss: 0.1031\n",
            "Epoch [731/800], Loss: 0.1015\n",
            "Epoch [733/800], Loss: 0.1001\n",
            "Epoch [735/800], Loss: 0.0986\n",
            "Epoch [737/800], Loss: 0.0972\n",
            "Epoch [739/800], Loss: 0.0958\n",
            "Epoch [741/800], Loss: 0.0944\n",
            "Epoch [743/800], Loss: 0.0931\n",
            "Epoch [745/800], Loss: 0.0917\n",
            "Epoch [747/800], Loss: 0.0905\n",
            "Epoch [749/800], Loss: 0.0892\n",
            "Epoch [751/800], Loss: 0.0879\n",
            "Epoch [753/800], Loss: 0.0867\n",
            "Epoch [755/800], Loss: 0.0855\n",
            "Epoch [757/800], Loss: 0.0843\n",
            "Epoch [759/800], Loss: 0.0832\n",
            "Epoch [761/800], Loss: 0.0821\n",
            "Epoch [763/800], Loss: 0.0810\n",
            "Epoch [765/800], Loss: 0.0799\n",
            "Epoch [767/800], Loss: 0.0789\n",
            "Epoch [769/800], Loss: 0.0779\n",
            "Epoch [771/800], Loss: 0.0769\n",
            "Epoch [773/800], Loss: 0.0759\n",
            "Epoch [775/800], Loss: 0.0749\n",
            "Epoch [777/800], Loss: 0.0739\n",
            "Epoch [779/800], Loss: 0.0729\n",
            "Epoch [781/800], Loss: 0.0720\n",
            "Epoch [783/800], Loss: 0.0711\n",
            "Epoch [785/800], Loss: 0.0702\n",
            "Epoch [787/800], Loss: 0.0694\n",
            "Epoch [789/800], Loss: 0.0685\n",
            "Epoch [791/800], Loss: 0.0677\n",
            "Epoch [793/800], Loss: 0.0669\n",
            "Epoch [795/800], Loss: 0.0661\n",
            "Epoch [797/800], Loss: 0.0653\n",
            "Epoch [799/800], Loss: 0.0645\n",
            "\n",
            "---\n",
            "Part 1 Score: 0.9949476462423499\n",
            "Part 1 Grade: 0.99 / 0.99 * 4 = 4.00\n"
          ]
        }
      ],
      "source": [
        "# DO NOT CHANGE\n",
        "# Getting the score and grade for Part 1\n",
        "\n",
        "score1 = score_policy.score_pos_bc(policy=POSBCRobot(), gui=False, model=None)\n",
        "grade1 = score1 / part1_bound * 4 if score1 < part1_bound else 4\n",
        "\n",
        "print('\\n---')\n",
        "print(f'Part 1 Score: {score1}')\n",
        "print(f'Part 1 Grade: {score1:.2f} / {part1_bound:.2f} * 4 = {grade1:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dx3AkUMamm4L"
      },
      "outputs": [],
      "source": [
        "# Optionally, uncomment and run the code below if you have saved an animation (gui = True) that you want to visualize.\n",
        "\n",
        "# Image(filename='part_1_anim.png', width=200, height=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3s-mYCwfZql_"
      },
      "source": [
        "# Part II. Behavioral cloning with visual observations\n",
        "\n",
        "In this part, you are asked to do a similar task as Part I, **but the observations will be RGB image observations of the world**, similar to the ones you used to do localization in Part III of Project 1. To process the RGB images, you will be implementing a CNN using PyTorch. [The official PyTorch tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) is a good starting point. As in Part I, the map that the robot is navigating is always the same. **This means that your model really only has to learn how to figure out where the robot and the goal are located, and how to navigate around a fixed set of obstacles.**\n",
        "\n",
        "All requirements from your code, as well as the evaluation method, are unchanged compared to Part I. The only difference is the nature of the observation that is provided to you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOmUffxGoCf5"
      },
      "source": [
        "**NOTES:**\n",
        "\n",
        "SHAPES AND DATA:\n",
        "- actions shape: (4000,)\n",
        "- obs shape: (4000, 64, 64, 3)\n",
        "- Shape of X (observations): (4000, 64, 64, 3)\n",
        "- Shape of y (actions): (4000,)\n",
        "- Unique actions in dataset: {0, 1, 2, 3}\n",
        "- First image shape: (64, 64, 3)\n",
        "- First action label: 0\n",
        "\n",
        "AFTER CONVERSION:\n",
        "- Converted X tensor shape: torch.Size([4000, 3, 64, 64])\n",
        "- Converted y tensor shape: torch.Size([4000])\n",
        "\n",
        "Observations (obs) → (4000, 64, 64, 3) → 4000 RGB images of size 64×64\n",
        "\n",
        "Actions (actions) → (4000,) → 4000 labels (one per image)\n",
        "\n",
        "For a Convolutional Neural Network (CNN), PyTorch expects the images in (N, C, H, W) format:\n",
        "\n",
        "- N = Number of samples (4000)\n",
        "- C = Channels (RGB → 3)\n",
        "- H = Height (64)\n",
        "- W = Width (64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWeyEjWCpBjI"
      },
      "source": [
        "**A typical CNN consists of:**\n",
        "\n",
        "- Convolutional Layers:\n",
        "    - Detect spatial patterns (edges, shapes).\n",
        "- Activation Functions (ReLU):\n",
        "    - Introduce non-linearity.\n",
        "- Pooling Layers (MaxPooling):\n",
        "    - Reduce spatial dimensions.\n",
        "- Fully Connected (FC) Layers:\n",
        "    - Classify the image into one of 4 actions.\n",
        "- Softmax or Logits Output:\n",
        "    - Convert model outputs into class probabilities."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import random\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "\n",
        "def set_seed(seed=42):\n",
        "   torch.manual_seed(seed)\n",
        "   torch.cuda.manual_seed_all(seed)  # Even though CPU, this doesn't hurt.\n",
        "   np.random.seed(seed\n",
        ")\n",
        "   random.seed(seed)\n",
        "   torch.backends.cudnn.deterministic = True\n",
        "   torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "\n",
        "class RGBBCRobot1(nn.Module):\n",
        "   def __init__(self, num_classes=4, lr=0.0001, betas=(0.9, 0.999), weight_decay=0):\n",
        "       super(RGBBCRobot1, self).__init__()\n",
        "\n",
        "\n",
        "       # Define CNN model using nn.Sequential\n",
        "       self.model = nn.Sequential(\n",
        "           # First Conv Block\n",
        "           nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
        "           nn.BatchNorm2d(32),\n",
        "           nn.LeakyReLU(negative_slope=0.05),\n",
        "           nn.MaxPool2d(2, 2),\n",
        "\n",
        "\n",
        "           # Second Conv Block\n",
        "           nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "           nn.BatchNorm2d(64),\n",
        "           nn.LeakyReLU(negative_slope=0.05),\n",
        "           nn.MaxPool2d(2, 2),\n",
        "\n",
        "\n",
        "           # Third Conv Block\n",
        "           nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "           nn.BatchNorm2d(128),\n",
        "           nn.LeakyReLU(negative_slope=0.05),\n",
        "           nn.MaxPool2d(2, 2),\n",
        "\n",
        "\n",
        "           # Flatten & Fully Connected Layers\n",
        "           nn.Flatten(),\n",
        "           nn.Linear(8192, 512),  # Adjusted for output size\n",
        "           nn.LeakyReLU(negative_slope=0.05),\n",
        "           #nn.Dropout(0.3),\n",
        "\n",
        "           nn.Linear(512, 256),\n",
        "           nn.LeakyReLU(negative_slope=0.05),\n",
        "           nn.Linear(256, num_classes)\n",
        "       )\n",
        "\n",
        "\n",
        "       # Loss Function\n",
        "       self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "       # Optimizer try .000198\n",
        "       #self.optimizer = optim.Adam(self.model.parameters(), lr=.000099, betas=(0.9, 0.999), weight_decay=0)\n",
        "       #batchs ize 30\n",
        "       self.optimizer = optim.Adam(self.model.parameters(), lr=.000045, betas=(0.9, 0.999), weight_decay=0)\n",
        "       #self.optimizer = optim.Adam(self.model.parameters(), lr=.00022, betas=(0.9, 0.999), weight_decay=0)\n",
        "       #BEST! num epochs 11 and batch size 30\n",
        "       #self.scheduler = StepLR(self.optimizer, step_size=5, gamma=0.45)\n",
        "       #self.scheduler = StepLR(self.optimizer, step_size=6, gamma=0.45)\n",
        "\n",
        "\n",
        "   def forward(self, x):\n",
        "       return self.model(x)\n",
        "\n",
        "\n",
        "   def train(self, data, num_epochs=12, batch_size=30):\n",
        "       \"\"\" Train model with Adam optimizer and optional scheduler \"\"\"\n",
        "       if isinstance(data, bool):\n",
        "           nn.Module.train(self, data)\n",
        "           return\n",
        "\n",
        "\n",
        "       # Extract images & labels\n",
        "       X, y = data['obs'], data['actions']\n",
        "       # Convert to tensors\n",
        "       X_tensor = torch.tensor(X, dtype=torch.float32).permute(0, 3, 1, 2)  # (N, 3, 64, 64)\n",
        "       y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "\n",
        "       # Create DataLoader\n",
        "       dataset = TensorDataset(X_tensor, y_tensor)\n",
        "       print(f\"[DEBUG] Converted X_tensor shape: {X_tensor.shape}\")\n",
        "       dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "       # Training Loop\n",
        "       for epoch in range(num_epochs):\n",
        "           total_loss = 0.0\n",
        "           for inputs, targets in dataloader:\n",
        "               self.optimizer.zero_grad()\n",
        "               outputs = self.forward(inputs)\n",
        "               loss = self.criterion(outputs, targets)\n",
        "               loss.backward()\n",
        "               self.optimizer.step()\n",
        "               total_loss += loss.item()\n",
        "           #self.scheduler.step()\n",
        "\n",
        "\n",
        "           avg_loss = total_loss / len(dataloader)\n",
        "           print(f\"Epoch [{epoch+1}/{num_epochs}] - Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "\n",
        "   def get_action(self, obs):\n",
        "    #\n",
        "\n",
        "      #.eval() removes the effects of any dropout layers\n",
        "       \"\"\" Get predicted action from observation \"\"\"\n",
        "       obs = obs * 255.0  # Scale input if needed\n",
        "\n",
        "       obs_tensor = torch.tensor(obs, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0)\n",
        "\n",
        "       with torch.no_grad():\n",
        "           predictions = self.forward(obs_tensor)\n",
        "           #instead of torch.max apply soft max = should give four probabilities one for each action\n",
        "           #sample from that distribution, scaling action based on how\n",
        "           #convert from the logits to probabilities\n",
        "           #_, action_position = torch.max(predictions, dim=1)\n",
        "           #action = action_position.item()\n",
        "\n",
        "\n",
        "           action_probabilities = F.softmax(predictions, dim=1)\n",
        "           #print(\"Action probabilities:\", action_probabilities[0].cpu().numpy())\n",
        "\n",
        "           action = torch.multinomial(action_probabilities[0], num_samples=1).item()\n",
        "\n",
        "       return action\n",
        "\n",
        "#rather than taking the maximum you can build in stoachstiity by softmaxing the output\n",
        "# Define hyperparameter grid including scheduler options\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MoTtr0DQfQXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cj5Xje--lkiZ"
      },
      "source": [
        "## Evaluation and Grading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2kSdH99oESr",
        "outputId": "1bbfe2c4-d328-47cf-f937-7a77ad0b58f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] Converted X_tensor shape: torch.Size([4000, 3, 64, 64])\n",
            "Epoch [1/12] - Avg Loss: 1.1412\n",
            "Epoch [2/12] - Avg Loss: 0.7588\n",
            "Epoch [3/12] - Avg Loss: 0.4710\n",
            "Epoch [4/12] - Avg Loss: 0.3678\n",
            "Epoch [5/12] - Avg Loss: 0.3146\n",
            "Epoch [6/12] - Avg Loss: 0.2770\n",
            "Epoch [7/12] - Avg Loss: 0.2413\n",
            "Epoch [8/12] - Avg Loss: 0.2130\n",
            "Epoch [9/12] - Avg Loss: 0.1925\n",
            "Epoch [10/12] - Avg Loss: 0.1719\n",
            "Epoch [11/12] - Avg Loss: 0.1525\n",
            "Epoch [12/12] - Avg Loss: 0.1401\n",
            "\n",
            "---\n",
            "Part 2 Score: 0.93581729749514\n",
            "Part 2 Grade: 0.94 / 0.95 * 5 = 4.93\n"
          ]
        }
      ],
      "source": [
        "# DO NOT CHANGE\n",
        "# Getting the score and grade for Part 2\n",
        "#Next should be the\n",
        "\n",
        "score2 = score_policy.score_rgb_bc1(policy=RGBBCRobot1(), gui=False, model=None)\n",
        "grade2 = score2 / part2_bound * 5 if score2 < part2_bound else 5\n",
        "\n",
        "print('\\n---')\n",
        "\n",
        "print(f'Part 2 Score: {score2}')\n",
        "print(f'Part 2 Grade: {score2:.2f} / {part2_bound:.2f} * 5 = {grade2:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TauW8ur-mhOV"
      },
      "outputs": [],
      "source": [
        "# Optionally, uncomment and run the code below if you have saved an animation (gui = True) that you want to visualize.\n",
        "\n",
        "# Image(filename='part_2_anim.png', width=200, height=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1I6v1TgCctRp"
      },
      "source": [
        "# Part III. Behavioral cloning with visual observations - multiple maps\n",
        "\n",
        "This part is the same as  Part II except that it is trained and tested differently. **The training set involves expert demonstrations for the two new obstacle maps. And while testing, for each trial, a different obstacle map is randomly selected.** This means that your model has to learn how to reason about what an obstacle is, and how to go around it, based on nothing more than an image. The main objective of this part is to show that, when using a CNN, it is possible for a model to achieve this. The evaluation method for this part is the same as Part I and II."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Leaky ReLU .01, Batch Size 16, Epochs 3; takes 8min\n",
        "- Drop out (.3) top fc layer\n",
        "  - lr: .000001 = 0.48\n",
        "  - lr: .00001 = 0.60\n",
        "  - lr: .0001 = 0.78\n",
        "  - lr: .0003 = 0.72\n",
        "  - lr: .00048 = 0.67\n",
        "  - lr: .00049 =\n",
        "  - lr: .0005 = 0.83\n",
        "  - lr: .00051 =\n",
        "  - lr: .00052 =\n",
        "  - lr: .00055 = 0.64\n",
        "  - lr: .0007 = 0.70\n",
        "  - lr: .001 = 0.63\n",
        "  - lr: .01 = 0.22\n",
        "\n",
        "- Drop out (.35)\n",
        "- lr: .0005 = 0.75\n",
        "\n",
        "- Drop out (.25)\n",
        "- lr: .0005 = .61\n",
        "\n",
        "- Drop out (.31)\n",
        "- lr: .0005 = .64\n",
        "\n",
        "nn.LeakyReLU(negative_slope=0.001), lr: .0005 ,\n",
        "Drop out(.30)\n",
        "- .68\n",
        "\n",
        "nn.LeakyReLU(negative_slope=0.05), lr: .0005\n",
        "- .56\n",
        "\n",
        "nn.LeakyReLU(negative_slope=0.005), lr: .0005\n",
        "- .70\n",
        "\n",
        "----------------------------------\n",
        "TWO LAYERS\n",
        " Dropout (.3) and (.3)\n",
        "- .71\n",
        "Dropout (.3) and (.1) lr = .0005\n",
        "- .63\n",
        "Dropout (.3) and (.1) lr = .001\n",
        "-"
      ],
      "metadata": {
        "id": "vzOXstLZOZnW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxQwN2MAdO09"
      },
      "outputs": [],
      "source": [
        "# Implement your solution for Part 3 below\n",
        "\n",
        "\n",
        "class RGBBCRobot2(RobotPolicy):\n",
        "\n",
        "    def train(self, data):\n",
        "        for key, val in data.items():\n",
        "            print(key, val.shape)\n",
        "        print(\"Using dummy solution for RGBBCRobot2\")\n",
        "        pass\n",
        "\n",
        "    def get_action(self, obs):\n",
        "    \treturn 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAky_Vu9l_EC"
      },
      "source": [
        "## Evaluation and Grading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmyAWRQwB2Rr"
      },
      "outputs": [],
      "source": [
        "# Optionally, uncomment and run the code below if you have saved an animation (gui = True) that you want to visualize.\n",
        "\n",
        "# Image(filename='part_3_anim.png', width=200, height=200)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class RGBBCRobot2(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "\n",
        "        super(RGBBCRobot2, self).__init__()  # Ensure parent constructor is called!\n",
        "        self.model = nn.Sequential(\n",
        "           # First Conv Block\n",
        "           nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
        "           nn.BatchNorm2d(32),\n",
        "           nn.LeakyReLU(negative_slope=0.01),\n",
        "           nn.MaxPool2d(2, 2),\n",
        "\n",
        "\n",
        "           # Second Conv Block\n",
        "           nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "           nn.BatchNorm2d(64),\n",
        "           nn.LeakyReLU(negative_slope=0.01),\n",
        "           nn.MaxPool2d(2, 2),\n",
        "\n",
        "\n",
        "           # Third Conv Block\n",
        "           nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "           nn.BatchNorm2d(128),\n",
        "           nn.LeakyReLU(negative_slope=0.01),\n",
        "           nn.MaxPool2d(2, 2),\n",
        "\n",
        "\n",
        "           # Flatten & Fully Connected Layers\n",
        "           nn.Flatten(),\n",
        "           nn.Linear(8192, 512),  # Adjusted for output size\n",
        "           nn.LeakyReLU(negative_slope=0.01),\n",
        "           #nn.Dropout(0.6),\n",
        "\n",
        "           nn.Linear(512, 256),\n",
        "           nn.LeakyReLU(negative_slope=0.01),\n",
        "           nn.Dropout(0.35),\n",
        "           nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "       # Loss Function\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        #self.optimizer = optim.Adam(self.model.parameters(), lr=.0005, betas=(0.9, 0.999), weight_decay=0)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=.0006, betas=(0.9, 0.99), weight_decay=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# 128 = .23\n",
        "# 64 = .239\n",
        "# 30 =  .26\n",
        "\n",
        "#DROP OUT IN ALL LAYERS\n",
        "#.03 = .309\n",
        "\n",
        "#In only bottom layer\n",
        "#0.6 = .268\n",
        "\n",
        "#DROP OUT IN TOP 3 CONV LAYERS\n",
        "#dropping the drop out to .2 in all three layers =  .304\n",
        "#dropping the drop out to .25 in all three layers =  .315\n",
        "#upping the drop out to .3 in all three layers = .3275\n",
        "#upping the drop out to .4 in all three layers = .20\n",
        "\n",
        "\n",
        "    def train(self, data, num_epochs=3, batch_size=15):\n",
        "\n",
        "        if isinstance(data, bool):\n",
        "            nn.Module.train(self, data)\n",
        "            return\n",
        "\n",
        "        # Extract images & labels\n",
        "        X, y = data['obs'], data['actions']\n",
        "\n",
        "        # Convert to tensors\n",
        "        X_tensor = torch.tensor(X, dtype=torch.float32).permute(0, 3, 1, 2)  # Shape: (N, 3, 64, 64)\n",
        "        y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "        # Create DataLoader\n",
        "        dataset = TensorDataset(X_tensor, y_tensor)\n",
        "        print(f\"[DEBUG] Converted X_tensor shape: {X_tensor.shape}\")\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        # Training Loop\n",
        "        for epoch in range(num_epochs):\n",
        "            total_loss = 0.0\n",
        "            for inputs, targets in dataloader:\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs = self.forward(inputs)\n",
        "                loss = self.criterion(outputs, targets)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            avg_loss = total_loss / len(dataloader)\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}] - Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    def get_action(self, obs):\n",
        "        \"\"\" Get predicted action from observation \"\"\"\n",
        "        #print(f\"[DEBUG] obs shape: {obs.shape}, type: {type(obs)}\")\n",
        "        #print(f\"[DEBUG] obs min: {obs.min()}, max: {obs.max()}\")\n",
        "\n",
        "        #self.model.eval()\n",
        "\n",
        "        obs = obs * 255.0  # If needed, scale input (ensure training and inference scaling match)\n",
        "\n",
        "        obs_tensor = torch.tensor(obs, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0)\n",
        "        #print(f\"[DEBUG] Converted obs_tensor shape: {obs_tensor.shape}\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predictions = self.forward(obs_tensor)\n",
        "            action_probabilities = F.softmax(predictions, dim=1)\n",
        "\n",
        "            action = torch.multinomial(action_probabilities[0], num_samples=1).item()\n",
        "\n",
        "        return action"
      ],
      "metadata": {
        "id": "bj0eB1pwdrzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PM4edyLqpT3v",
        "outputId": "2da3a7d6-85d1-4d29-ae7b-bc3b1c85428b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] Converted X_tensor shape: torch.Size([12000, 3, 64, 64])\n",
            "Epoch [1/3] - Avg Loss: 0.6861\n",
            "Epoch [2/3] - Avg Loss: 0.3333\n",
            "Epoch [3/3] - Avg Loss: 0.2903\n",
            "\n",
            "---\n",
            "Part 3 Score: 0.7690467490816303\n",
            "Part 3 Grade: 0.77 / 0.95 * 6 = 4.86\n"
          ]
        }
      ],
      "source": [
        "# DO NOT CHANGE\n",
        "# Getting the score and grade for Part 3\n",
        "\n",
        "score3 = score_policy.score_rgb_bc2(policy=RGBBCRobot2(), gui=False, model=None)\n",
        "grade3 = score3 / part3_bound * 6 if score3 < part3_bound else 6\n",
        "\n",
        "print('\\n---')\n",
        "print(f'Part 3 Score: {score3}')\n",
        "print(f'Part 3 Grade: {score3:.2f} / {part3_bound:.2f} * 6 = {grade3:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNJs1PcRpnSw"
      },
      "source": [
        "# Other Requirements and Hints\n",
        "\n",
        "- **Training time**: To keep auto-grading feasible, your total training time must be strictly under 3 mins, 15mins, and 10 mins for parts 1, 2, and 3. These time budgets are more than enough to achieve full credits on this project. Note that longer training time does not necessarily mean higher performance because of overfitting. The faster your network trains the better!\n",
        "- **Memory usage**: Make sure your code does not require too much memory. The required amount of RAM for this assignment should not be more than 8GB.\n",
        "- **NO GPU**: No GPU is required or allowed for this assignment.\n",
        "- **Reproducibility**: We have ensured that the randomness of the environment is deterministic. To get reproducible scores you must ensure your model training and prediction are also reproducible. The randomly initialized weights of the neural network should be made repeatable using seeding. You can add PyTorch seeding method below and see [PyTorch Reproducibility](https://pytorch.org/docs/stable/notes/randomness.html) to learn more.\n",
        "  ```\n",
        "  import torch\n",
        "  torch.manual_seed(0)\n",
        "  ```\n",
        "- **Classifier**: In all the parts we are training a neural network to solve a classification problem and it is important to use a reasonable loss function. For example, the MSE (mean squared classification) error has drawbacks related to sensitivity. Cross entropy loss usually has good performance for classification tasks and you can find the documentation for it [here](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) and is further explained below. However, note that you are free to use any loss function you like.\n",
        "  - Cross entropy is a concept from information theory which is defined for two probability distributions. Cross entropy is minimum when the two distributions involved are the same and this is the property that makes it useful as a loss function in the context of machine learning. The idea is to minimize the cross entropy between the prediction distribution and the label distribution. For our case where we are training a neural network for classification, we can have the network output a score for each action. Cross entropy can be computed from these scores by converting to probability values (using softmax) and comparing it with the label distribution. The label distribution is obtained simply by assigning a probability of 1 to ground truth action and 0 to all other actions. Once trained, the best action can found by just choosing the action with the highest probability (i.e., the highest score) as predicted by the network.\n",
        "- **Optimizer**: While it is possible to use a simple optimizer to achieve the desired accuracy, the training time can be quite high. There exist a number of optimizers implemented in PyTorch that have much faster convergence.\n",
        "- **Parameter tuning**: Keep your architectures simple and slowly add complexity (more layers/kernels) to improve accuracy. Remember \"To Err is Human\" and the expert data (collected by a human) that you are training on is not perfect. Having a 100% training accuracy (very small training loss) might not be the best for achieving the highest score. So make sure your model does not overfit during training.\n",
        "- **PyTorch input shape**: Notice that the expected input shape to CONV2D in PyTorch is (N, C, H, W), where N is the batch size, C is the number of channels, H is the image height and W is the image width. You will need to switch axes for the incoming images in order for them to be correctly passed to the first convolution layer."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}